dataset:
  name: FB15k-237

training:
  epochs: 10000  # Limit the number of training epochs
  graph_batch_size: 30000  # Number of triples to randomly sample during training
  negative_sampling:
    sampling_rate: 10  # Number of negative samples to produce per triple
    head_prob: 0.5  # Probability of corrupting heads (i.e. 0.5 means corrupt 50% heads and 50% tails)head_prob: 0.5  # Ratio of corrupting heads (i.e. 0.5 means corrupt 50% heads and 50% tails)
  optimiser:
    algorithm: adam
    weight_decay: 0.0
    learn_rate: 0.01
  gradient_clip_value: 1.0  # Clip value for Gradient Clipping
  use_cuda: False  # If true, model is trained on GPU

encoder:
  model: rgcn
  num_layers: 2  # Number of graph convolution layers
  node_embedding: 500  # Size of node embedding
  hidden1_size: 500  # Size of first hidden layer
  hidden2_size: 500  # Size of second hidden layer
  decomposition:
    type: block
    num_blocks: 5
  edge_dropout:
    general: 0.5  # Dropout rate for all edges (except self-loops)
    self_loop: 0.2  # Dropout rate for self-loops

decoder:
  model: distmult
  l2_penalty: 0.01

evaluation:
  final_run: True  # If true, evaluates model on test set. Otherwise, validation set is used.
  filtered: True  # If true, reports filtered metrics. Otherwise, raw metrics are computed.
  early_stopping:
    check_every: 500  # Evaluate model performance at every n epoch interval
    metric: mrr
    min_epochs: 1000  # Minimum of epochs before early stopping can be applied
    num_stops: 1  # If no improvement in metric is detected after n times, apply early stopping
    eval_size: 500  # Number of triples to randomly sample from training dataset
#  final_eval_size: 10000  # Limit the number of triples to randomly sample from test dataset
