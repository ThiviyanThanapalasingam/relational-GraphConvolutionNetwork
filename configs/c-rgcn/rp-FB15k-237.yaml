dataset:
  name: FB15k-237

training:
  epochs: 10000  # Limit the number of training epochs
  neg_sample_rate: 10  # Number of negative samples to produce per triple
  optimiser:
    algorithm: adam
    weight_decay: 0.0
    learn_rate: 0.01
  use_cuda: True  # If true, model is trained on GPU

encoder:
  model: c-rgcn
  node_embedding_l2_penalty: 0.005  # l2 penalty on node embeddings
  num_layers: 2  # Number of graph convolution layers
  embedding_size: 128  # Size for node and relation embedding
  hidden1_size: 10  # Size of first hidden layer - Dimension to compress node embeddings to (ideally much smaller than embedding_size)
  edge_dropout:
    general: 0.5  # Dropout rate for all edges (except self-loops)
    self_loop: 0.2  # Dropout rate for self-loops

decoder:
  model: distmult
  l2_penalty: 0.01

evaluation:
  final_run: False  # If true, evaluates model on test set. Otherwise, validation set is used
  filtered: True  # If true, reports filtered metrics. Otherwise, raw metrics are computed
  early_stopping:
    check_every: 500  # Evaluate model performance at every n epoch interval
    metric: mrr
    min_epochs: 1000  # Minimum of epochs before early stopping can be applied
    num_stops: 1  # If no improvement in metric is detected after n times, apply early stopping
    eval_size: 500  # Number of triples to randomly sample from training dataset
#  final_eval_size: 10000  # Limit the number of triples to randomly sample from test dataset
